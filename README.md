# NoiseReduction4DeepLearning
NoiseReduction4DeepLearning - It automates the speech segment and noise reduction process mainly used for Deep Learning dataset

## WorkFlow 정리 :

# 0. 크롤링 작업 :

펭수 영상(90개), 양희은 라디오 음성(1000)개 크롤링. 파이썬 셀레니엄, 웹브라우저 모듈이용.

---

# 1. 음악 소리, 배경음악 제거 작업(Spleeter) : 

(이걸 어떻게 해야할까?) -> 배경음악은 spleeter 이용. 일단 펭수 유투브에서 잘라놓음(폴더로 옮겨 놓음).  양희은 목소리는 1000개 파일 40분 가량 데이터 크롤링해 놓음. 펭수처럼 배경음악 제거 작업 진행중.
RNN 모델을 활용하고 있으며, MusDB 라이브러리를 사용해서 사용자가 추가적으로 해당 모델을 학습시킬 수 있다. -> 성능향상의 여지 있음.
+ 양희은 라디오 음성 크롤링 후, 음악 제거 작업 완료

---

# 2. inaSpeechSegmenter : 스피치(남/여), 뮤직, 노이즈 이 3가지를 잘 분리해주는 모델.

펭수 목소리 분리해서, csv파일로  start, end time정리한 값을 받는다(ina_speech_segmenter.py 파일 수정 및 indexing_data.py 스크립트 작성). 이때, 그냥 media 폴더에 펭수 유투브 영상 82개 (파일명 안 바꾸고 그냥 통채로) 넣어버리면 알아서 작업해 준다. -> CNN을 기반으로 한 모델을 활용했다.
+ 양희은 라디오 음성 음악제거한 파일을 다시 Female음성만 분리해서 저장해 놓았다.

---

#3. Audacity 방식으로 효과음 제거하기? 혹은 SFX의 특징을 잡아내서 지워버리는 건 어떨까? : 

Numpy array 로 변환을 시킨 다음에, SFX의 Array 특징을 잡아내서 제거하는 코드를 짜볼 수도 있음. 그런데 소리가 중첩될 경우, 해당하는 SFX효과의 Numpy array만 제거한다는 것이 어떤 의미인지 조금 연구를 해봐야 함.

Audacity 방식의 경우 조금 더 구현체를 찾아보고 매칭만 잘 시키면 SFX효과를 삭제할 수 있지 않을까? -> 이것도 리서치를 해 봐야 한다!

-> Audacity 방식을 시도해 보았으나 성능이 매우 좋지 않다. 또한 노이즈 소스를 미리 알고 있어야 하며, 해당 노이즈가 섞여 들어간 정확한 타이밍을 알아야 한다.

-> Low pass filtering 을 적용해 보았으나, 정성적으로는 효과가 좋지 않다. 전체적으로 데시벨 크기가 줄어들고, 특정 잡음이 생기는 대역을 분리하는 것은 무리가 있다

-> notch filter(특정 주파수 밴드만 통과시키는 방식의 필터링) : 이걸 적용하면 원하는 노이즈 패턴을 분석했을 때, 상대적으로 효과적으로 특정 피치의 잡음을 제거할 수 있다! (예: 6000Hz + Q인자<잘라낼 노치의 폭> 기준으로  6000Hz 근처의 음역대의 소리를 제거할 수 있음을 확인했다. : 하지만 어떤 음역대를 잘라야 해당 브금/노이즈를 제거할 수 있을지는 좀더 시도해 보아야 한다. 수작업이 필요할 수도?
-> 이거 이해하려면 DSP 공부해야.
-> 아니 근데 Notch 돌려도 별 효과가 없는 듯? 결국 각 브금마다 frequency band를 다르게 설정해야 하잖아?

---

4. csv_to_sliced.py : 이 파일을 돌리면, 일단 media 폴더에 들어있는 원본으로부터 펭수 목소리를 대강 잘라서 다시 dataset_sliced 폴더에 넣어준다! : pydub 라이브러리를 사용했다.

---

5. 화자분리(Speaker 분리) :
일단 resemblyzer라는 좋은 패키지가 있어서 사용해 보기. -> 아직 여기까지 안 갔다.

---

6. 음성 파일에 맞는 스크립트 생성 : 
 Speech-To-Text API 이용해서 스크립트 작성 -> 마이크로소프트 SDK 사용해서 데이터셋 만들기
